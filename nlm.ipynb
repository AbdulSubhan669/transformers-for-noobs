{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdulSubhan669/transformers-for-noobs/blob/main/nlm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "JwLHAOx-yw2O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import math\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "nMufh2guBbwT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "cfbe0e16-8997-491a-9784-c3405f16ccce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!pip install datasets\\nfrom datasets import load_dataset\\nds = load_dataset(\"princeton-nlp/datasets-for-simcse\")\\ndataset = ds[\\'train\\']'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 210
        }
      ],
      "source": [
        "\"\"\"!pip install datasets\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"princeton-nlp/datasets-for-simcse\")\n",
        "dataset = ds['train']\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = {'text': ['YMCA in South Australia', \"South Australia (SA) \\xa0has a unique position in Australia's history as, unlike the other states which were founded as colonies, South Australia began as a self governing province Many were attracted to this and Adelaide and SA developed as an independent and free thinking state.\", 'The compound of philosophical radicalism, evangelical religion and self reliant ability typical of its founders had given an equalitarian flavour to South Australian thinking from the beginning.', 'It was into this social setting that in February 1850 a meeting was called primarily for the formation of an Association (apparently meaning a Y.M.C.A.)', \"for apprentices and others, after their day's work, to enjoy books, lectures, discussions, readings, friendly relief and recreation for a leisure hour.\", 'In September 1850 records show that this became “The Young Men\\'s Christian Association of South Australia\" as evidenced by a member\\'s letter in London Y.M.C.A.', 'Report 1851.', 'There was no census in 1850 but the 1851 census put the total population of South Australia at 63,700 with males numbering 35,302.', 'The discovery of Gold in Ballarat caused a large migration from South Australia and by 1852 some 8000 had left for the Goldfields.', 'As a consequence the various YMCA groups that had become established failed and by 1870 none remained.']}"
      ],
      "metadata": {
        "id": "79YuaAO_QiVO"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BPETokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.token_to_index = {}\n",
        "        self.index_to_token = {}\n",
        "\n",
        "    def initialize_vocab(self, dataset, batch_size=10000):\n",
        "\n",
        "        vocab = defaultdict(int)\n",
        "\n",
        "\n",
        "        for i in range(len(dataset['text'])):\n",
        "            sentence = dataset['text'][i]\n",
        "            words = sentence.strip().split()\n",
        "            for word in words:\n",
        "                vocab[word] += 1\n",
        "                #vocab[word + '</w>'] += 1\n",
        "\n",
        "        self.vocab = vocab\n",
        "        print(f\"Initialized Vocab: {dict(list(self.vocab.items())[:10])}\")\n",
        "\n",
        "    def get_stats(self):\n",
        "\n",
        "        pairs = defaultdict(int)\n",
        "        for word, freq in self.vocab.items():\n",
        "            symbols = word.split()\n",
        "            for i in range(len(symbols) - 1):\n",
        "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "        return pairs\n",
        "\n",
        "    def merge_vocab(self, pair):\n",
        "\n",
        "        new_vocab = {}\n",
        "        bigram = re.escape(' '.join(pair))\n",
        "        pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "\n",
        "        for word in self.vocab:\n",
        "            new_word = pattern.sub(''.join(pair), word)\n",
        "            new_vocab[new_word] = self.vocab[word]\n",
        "\n",
        "        self.vocab = new_vocab\n",
        "\n",
        "    def perform_bpe(self, num_merges):\n",
        "\n",
        "        for i in range(num_merges):\n",
        "            pairs = self.get_stats()\n",
        "            if not pairs:\n",
        "                break\n",
        "            best_pair = max(pairs, key=pairs.get)\n",
        "            self.merge_vocab(best_pair)\n",
        "            print(f\"Iteration {i+1}: Merged {best_pair}\")\n",
        "            print(f\"Updated Vocab: {dict(list(self.vocab.items())[:10])}\")\n",
        "\n",
        "    def build_token_to_index(self):\n",
        "\n",
        "        for idx, token in enumerate(self.vocab.keys()):\n",
        "            self.token_to_index[token] = idx\n",
        "            self.index_to_token[idx] = token\n",
        "\n",
        "        return self.token_to_index\n",
        "\n",
        "    def tokenize(self,text):\n",
        "      tokens = []\n",
        "      print(f\"Token to Index Mapping: {self.token_to_index}\")\n",
        "      for word in text.split():\n",
        "        #token = word + \"</w>\"\n",
        "        token = word\n",
        "        #print(f\"Trying to match: {token}\")\n",
        "        #print(f\"Generated token: '{token}'\")\n",
        "        if token in self.token_to_index:\n",
        "          tokens.append(self.token_to_index[token])\n",
        "        else:\n",
        "           print(\"________\")\n",
        "\n",
        "      return tokens\n",
        "\n",
        "\n",
        "\n",
        "bpe_tokenizer = BPETokenizer()\n",
        "bpe_tokenizer.initialize_vocab(dataset)\n",
        "bpe_tokenizer.perform_bpe(num_merges=500)\n",
        "token_to_index = bpe_tokenizer.build_token_to_index()\n"
      ],
      "metadata": {
        "id": "4bJctlX2993O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "6030f13f-3076-4cf8-b1ea-e7fdc3ab430e"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized Vocab: {'YMCA': 2, 'in': 6, 'South': 7, 'Australia': 5, '(SA)': 1, 'has': 1, 'a': 8, 'unique': 1, 'position': 1, \"Australia's\": 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,embedding_dim, max_length = 5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.encoding = torch.zeros(max_length, embedding_dim)\n",
        "    position = torch.arange(0, max_length).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / embedding_dim))\n",
        "    self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "    self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "    self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return x + self.encoding[:, :x.size(1)]\n"
      ],
      "metadata": {
        "id": "yFHs1EJ1wdyF"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mistral_Attention(nn.Module):\n",
        "  def __init__(self,embedding_dim, num_heads):\n",
        "    super(Mistral_Attention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.head_dim = embedding_dim // num_heads\n",
        "\n",
        "    assert (\n",
        "        self.head_dim * num_heads == embedding_dim\n",
        "    ), \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "\n",
        "    self.q = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.k = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.v = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.out = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    batch_size, seq_length, _ = x.size()\n",
        "\n",
        "    query = self.q(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1,2)\n",
        "    key = self.k(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1,2)\n",
        "    value = self.v(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1,2)\n",
        "\n",
        "\n",
        "    attention_scores = torch.matmul(query, key.transpose(-2, -1)) / self.head_dim ** 0.5\n",
        "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "    context = torch.matmul(attention_weights, value)\n",
        "    context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embedding_dim)\n",
        "    return self.out(context)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4NfCmvq-9pjg",
        "collapsed": true
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, ff_dim):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.attention = Mistral_Attention(embedding_dim, num_heads)\n",
        "        self.feed_forward_1 = nn.Linear(embedding_dim, ff_dim)  # First layer of FFN\n",
        "        self.feed_forward_2 = nn.Linear(ff_dim, embedding_dim)  # Project back to embedding_dim\n",
        "        self.output_layer = nn.Linear(embedding_dim, vocab_size)  # Adjust according to your needs\n",
        "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embedding_dim)  # Should match embedding_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
        "        attention_output = self.attention(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
        "        x = self.layer_norm1(attention_output + x)  # Add & Norm\n",
        "        ff_output = self.feed_forward_1(x)  # Shape: (batch_size, seq_length, ff_dim)\n",
        "        ff_output = self.feed_forward_2(ff_output)  # Shape: (batch_size, seq_length, embedding_dim)\n",
        "        x = self.layer_norm2(ff_output + x)  # Add & Norm\n",
        "        return self.output_layer(x)  # Final output\n",
        "\n",
        "    def predict(self, input_tokens, max_length=20):\n",
        "        \"\"\"\n",
        "        Generate new tokens based on the input tokens.\n",
        "\n",
        "        :param input_tokens: Initial tokens for generation (tensor).\n",
        "        :param max_length: Maximum length of the generated sequence.\n",
        "        :return: Generated token sequence.\n",
        "        \"\"\"\n",
        "        self.eval()  # Set the model to evaluation mode\n",
        "        generated_tokens = input_tokens.tolist()  # Store input tokens for generating output\n",
        "\n",
        "        # Start generating tokens\n",
        "        for _ in range(max_length):\n",
        "            input_tensor = torch.tensor(generated_tokens).unsqueeze(0)  # Shape: (1, current_length)\n",
        "            with torch.no_grad():\n",
        "                output = self.forward(input_tensor)  # Forward pass\n",
        "\n",
        "            # Get the logits for the last token\n",
        "            last_token_logits = output[0, -1, :]  # Shape: (vocab_size,)\n",
        "            probs = torch.softmax(last_token_logits, dim=-1)  # Get probabilities\n",
        "\n",
        "            # Sample the next token (you can change this to argmax for greedy decoding)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).item()  # Sample next token\n",
        "\n",
        "            generated_tokens.append(next_token)  # Add the new token to the sequence\n",
        "\n",
        "        return generated_tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "GwRdoj6Gw1Pm"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "vocab_size = len(bpe_tokenizer.token_to_index)\n",
        "embedding_dim = 128\n",
        "num_heads = 8\n",
        "ff_dim = 512\n",
        "\n",
        "model = Transformer(vocab_size, embedding_dim, num_heads, ff_dim)\n",
        "\n",
        "\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyj9WsI2Ob40",
        "outputId": "fd3912c9-1941-4884-e869-0f29bad4c4ba"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (embedding): Embedding(143, 128)\n",
              "  (attention): Mistral_Attention(\n",
              "    (q): Linear(in_features=128, out_features=128, bias=True)\n",
              "    (k): Linear(in_features=128, out_features=128, bias=True)\n",
              "    (v): Linear(in_features=128, out_features=128, bias=True)\n",
              "    (out): Linear(in_features=128, out_features=128, bias=True)\n",
              "  )\n",
              "  (feed_forward_1): Linear(in_features=128, out_features=512, bias=True)\n",
              "  (feed_forward_2): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (output_layer): Linear(in_features=128, out_features=143, bias=True)\n",
              "  (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "seed_text = \"Once upon a time in Austrailia\"\n",
        "seed_tokens = bpe_tokenizer.tokenize(seed_text)\n",
        "print(f\"Seed Tokens: {seed_tokens}\")\n",
        "\n",
        "\n",
        "max_length = 100\n",
        "generated_tokens = model.predict(torch.tensor(seed_tokens), max_length=max_length)\n",
        "print(f\"Generated Tokens: {generated_tokens}\")\n",
        "\n",
        "\n",
        "generated_text = ' '.join(bpe_tokenizer.index_to_token[token] for token in generated_tokens)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1NApvPfUrj_",
        "outputId": "d2f2f1e8-4b46-4a79-f5ae-fcdd90758377"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token to Index Mapping: {'YMCA': 0, 'in': 1, 'South': 2, 'Australia': 3, '(SA)': 4, 'has': 5, 'a': 6, 'unique': 7, 'position': 8, \"Australia's\": 9, 'history': 10, 'as,': 11, 'unlike': 12, 'the': 13, 'other': 14, 'states': 15, 'which': 16, 'were': 17, 'founded': 18, 'as': 19, 'colonies,': 20, 'began': 21, 'self': 22, 'governing': 23, 'province': 24, 'Many': 25, 'attracted': 26, 'to': 27, 'this': 28, 'and': 29, 'Adelaide': 30, 'SA': 31, 'developed': 32, 'an': 33, 'independent': 34, 'free': 35, 'thinking': 36, 'state.': 37, 'The': 38, 'compound': 39, 'of': 40, 'philosophical': 41, 'radicalism,': 42, 'evangelical': 43, 'religion': 44, 'reliant': 45, 'ability': 46, 'typical': 47, 'its': 48, 'founders': 49, 'had': 50, 'given': 51, 'equalitarian': 52, 'flavour': 53, 'Australian': 54, 'from': 55, 'beginning.': 56, 'It': 57, 'was': 58, 'into': 59, 'social': 60, 'setting': 61, 'that': 62, 'February': 63, '1850': 64, 'meeting': 65, 'called': 66, 'primarily': 67, 'for': 68, 'formation': 69, 'Association': 70, '(apparently': 71, 'meaning': 72, 'Y.M.C.A.)': 73, 'apprentices': 74, 'others,': 75, 'after': 76, 'their': 77, \"day's\": 78, 'work,': 79, 'enjoy': 80, 'books,': 81, 'lectures,': 82, 'discussions,': 83, 'readings,': 84, 'friendly': 85, 'relief': 86, 'recreation': 87, 'leisure': 88, 'hour.': 89, 'In': 90, 'September': 91, 'records': 92, 'show': 93, 'became': 94, '“The': 95, 'Young': 96, \"Men's\": 97, 'Christian': 98, 'Australia\"': 99, 'evidenced': 100, 'by': 101, \"member's\": 102, 'letter': 103, 'London': 104, 'Y.M.C.A.': 105, 'Report': 106, '1851.': 107, 'There': 108, 'no': 109, 'census': 110, 'but': 111, '1851': 112, 'put': 113, 'total': 114, 'population': 115, 'at': 116, '63,700': 117, 'with': 118, 'males': 119, 'numbering': 120, '35,302.': 121, 'discovery': 122, 'Gold': 123, 'Ballarat': 124, 'caused': 125, 'large': 126, 'migration': 127, '1852': 128, 'some': 129, '8000': 130, 'left': 131, 'Goldfields.': 132, 'As': 133, 'consequence': 134, 'various': 135, 'groups': 136, 'become': 137, 'established': 138, 'failed': 139, '1870': 140, 'none': 141, 'remained.': 142}\n",
            "________\n",
            "________\n",
            "________\n",
            "________\n",
            "Seed Tokens: [6, 1]\n",
            "Generated Tokens: [6, 1, 52, 113, 65, 10, 71, 97, 28, 105, 88, 77, 26, 0, 104, 30, 22, 87, 12, 51, 12, 29, 15, 49, 64, 115, 115, 76, 97, 74, 121, 23, 109, 132, 62, 99, 87, 60, 76, 87, 79, 78, 128, 47, 7, 112, 72, 93, 0, 67, 18, 7, 63, 87, 30, 114, 141, 49, 141, 66, 49, 36, 6, 63, 142, 41, 88, 66, 70, 114, 128, 120, 123, 106, 15, 117, 36, 16, 16, 34, 75, 51, 135, 63, 114, 25, 9, 74, 22, 110, 98, 57, 140, 68, 110, 90, 20, 129, 100, 28, 118, 142]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Generated Text: {generated_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pLaPHMAePit",
        "outputId": "8ff72720-9e76-4a34-9ca8-418c108c6362"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: a in equalitarian put meeting history (apparently Men's this Y.M.C.A. leisure their attracted YMCA London Adelaide self recreation unlike given unlike and states founders 1850 population population after Men's apprentices 35,302. governing no Goldfields. that Australia\" recreation social after recreation work, day's 1852 typical unique 1851 meaning show YMCA primarily founded unique February recreation Adelaide total none founders none called founders thinking a February remained. philosophical leisure called Association total 1852 numbering Gold Report states 63,700 thinking which which independent others, given various February total Many Australia's apprentices self census Christian It 1870 for census In colonies, some evidenced this with remained.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHQf3bw1bQMze63mf02gTN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}